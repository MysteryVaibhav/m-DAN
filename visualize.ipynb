{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max caption len: 174\n"
     ]
    }
   ],
   "source": [
    "from model import mDAN\n",
    "from process_data import *\n",
    "from util import *\n",
    "import os.path\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Load model\n",
    "model = mDAN()\n",
    "model.load_state_dict(torch.load('model_weights_38_0.259.t7'))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Get the mapping\n",
    "img_caption, max_len, id_img = get_captions()\n",
    "word_freq = frequency_map(img_caption)\n",
    "word_idx = construct_vocab(word_freq, 5)\n",
    "\n",
    "# Enter the caption:\n",
    "orig_caption = \"People playing with a ball in the park\"\n",
    "\n",
    "# Repeating the caption 5 times to fill padding\n",
    "caption = orig_caption\n",
    "caption += ' ' + orig_caption\n",
    "caption += ' ' + orig_caption\n",
    "caption += ' ' + orig_caption\n",
    "caption += ' ' + orig_caption\n",
    "\n",
    "# Get the encoded caption\n",
    "one_hot, mask = encode_caption([x for x in caption.lower().split(\" \")], word_idx, max_len)\n",
    "\n",
    "# Get the visual context vectors for all the images:\n",
    "if os.path.exists('all_visual_context_features.npy'):\n",
    "    all_z_v = np.load('all_visual_context_features.npy')\n",
    "else:\n",
    "    all_z_v = None  # no_of_images * visual_context_features\n",
    "    dummy_caption = ' '\n",
    "    dummy_one_hot, dummy_mask = encode_caption([x for x in dummy_caption.lower().split(\" \")], word_idx, max_len)\n",
    "    i = 0\n",
    "    for img, _ in img_caption.items():\n",
    "        image = np.load(TRAIN_IMAGES_DIR + \"{}.npy\".format(img)).reshape((NO_OF_REGIONS_IN_IMAGE, VISUAL_FEATURE_DIMENSION))\n",
    "        # image = np.random.random((NO_OF_REGIONS_IN_IMAGE, VISUAL_FEATURE_DIMENSION))\n",
    "        _, _, z_v = model(to_variable(to_tensor(dummy_one_hot).long().unsqueeze(0)),\n",
    "                          to_variable(to_tensor(dummy_mask).unsqueeze(0)),\n",
    "                          to_variable(to_tensor(image).unsqueeze(0)), True)\n",
    "        if all_z_v is None:\n",
    "            all_z_v = z_v.data.cpu()\n",
    "        else:\n",
    "            all_z_v = torch.cat((all_z_v, z_v.data.cpu()), 0)\n",
    "#         i += 1\n",
    "#         if i == 10:\n",
    "#             break\n",
    "    all_z_v = all_z_v.numpy()\n",
    "    np.save('all_visual_context_features.npy', all_z_v)\n",
    "\n",
    "# Get the textual context vector for the input caption\n",
    "dummy_image = np.random.random((NO_OF_REGIONS_IN_IMAGE, VISUAL_FEATURE_DIMENSION))\n",
    "_, z_u, _ = model(to_variable(to_tensor(one_hot).long().unsqueeze(0)), to_variable(to_tensor(mask).unsqueeze(0)),\n",
    "                  to_variable(to_tensor(dummy_image).unsqueeze(0)), True)\n",
    "z_u = z_u.data.cpu().numpy()\n",
    "\n",
    "# Compute similarity with the existing images\n",
    "similarity = np.matmul(all_z_v, z_u.T)\n",
    "top_5_img_idx = (-similarity.squeeze(axis=1)).argsort()[:5]\n",
    "\n",
    "# Fetch all images\n",
    "for i in top_5_img_idx:\n",
    "    #print(id_img[i])\n",
    "    img = mpimg.imread(IMAGES_DIR + \"{}.jpg\".format(id_img[i]))\n",
    "    plt.figure()\n",
    "    plt.imshow(img)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
